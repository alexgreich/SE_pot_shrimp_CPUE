---
title: "District 7"
author: "Alex Reich"
date: "2024-04-24"
output: html_document
---
Aside:
Argh, I had to upgrade to R 4.4.0 to get lme4 (the matrix package, specifically) to work. But now the r markdown settings are different and I can't default to the home directory being the project. So I will set that here:
```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/agreich/OneDrive - State of Alaska/Desktop/Shrimp/Shrimp tickets (pot shrimp)/SE_pot_shrimp_CPUE')
```



Anyway, let's get started. This document will look at management unit District 7 for pot (spot) shrimp standardization. It will load in the (pre-wrangled) data, wrangle further for district 1, and then some EDA (focusing on variables of interest and correlation). The analysis will be by district with an HGAM (hierarchical GAM) with the variables of interest as fixed effects and Analysis Area as a random effect. I will do model selection on a few versions of this HGAM. I will then make graphs of the standardized CPUE (by district and) and whatever else Max wants. THEN, I will run individual models by analysis areas, ideally with an automated function, and compare the graphs of those automated functions to the cpue of the ranef model.This will help me decide if models should be by MANAGEMENT UNIT (larger) or by ANALYSIS AREA (smaller). In hindsight, I should start with District 7, since I have explored the Upper Ernest sound data extensively.Ok, now the RMD is named accordingly 


Load libraries
```{r}
library(tidyverse)
library(ggplot2)
library(cowplot)
library(viridis)
library(RColorBrewer)
library(mgcv)
```


Load the data and select for D7
```{r}
wrangled_shrimp <- read.csv("Data/wrangled shrimp focus years.csv")
D7 <- wrangled_shrimp %>% filter(Management_unit == "District 7") %>%
  mutate(Analysis.Area = factor(Analysis.Area),
         ADFG.Number = factor(ADFG.Number),
         Season.Ref = factor(Season.Ref)
         )#make things factors for the gams

unique(D7$Analysis.Area)

```


District 7 contains analysis areas:
  Bradfield Canal
  Lower Ernest Sound
  Upper Ernest Sound
  Zimovia Strait
  
  
  
## Exploratory Data Analysis for D7
Variables of interest:
Random effect: Analysis.Area
Fixed effects: jdate (smoothed and... cyclical?), vessel # (the count of vessels), ADFG # (individual vessel (fixed or random effect??)), Season aka year, 
--If I do not include jdate, simplifies things?
Response: nominal cpue

Q: should I do something besides a log-transformation?
THIN OUT
```{r}

#cpue dist
ggplot(D7) + aes(x=CPUE_nom) + geom_density()
ggplot(D7) + aes(x=log(CPUE_nom+0.001)) + geom_density()
qqnorm(log(D7$CPUE_nom)) #that kind of looks bad. Maybe somehting other than log-trans?
qqnorm(D7$CPUE_nom)

ggplot(D7) + aes(x=CPUE_nom) + geom_density()+facet_wrap(~Analysis.Area)
ggplot(D7) + aes(x=log(CPUE_nom+0.001)) + geom_density()+facet_wrap(~Analysis.Area)

#cpue by year
ggplot(D7) + aes(x=factor(Season.Ref), y=CPUE_nom) + geom_point() #ther is a big outlier
ggplot(D7) + aes(x=factor(Season.Ref), y=CPUE_nom) + geom_boxplot(outliers = F)

ggplot(D7) + aes(x=factor(Season.Ref), y=CPUE_nom) + geom_boxplot(outliers = F) + facet_wrap(~Analysis.Area) #cpue varies... cyclically with time. Is season a fixed effect? I want to estimate it, so probs not. its def not linear.

#catch by year
ggplot(D7) + aes(x=factor(Season.Ref), y=total_weight) + geom_boxplot() #this is total weight per fish ticket.
ggplot(D7) + aes(x=factor(Season.Ref), y=max_pots_2) + geom_boxplot()
ggplot(D7) + aes(x=factor(Season.Ref), y=max_pots_2) + geom_boxplot(outliers=F)

ggplot(D7) + aes(x=factor(Season.Ref), y=total_weight) + geom_boxplot() + facet_wrap(~Analysis.Area) #max pots per fish ticket
ggplot(D7) + aes(x=factor(Season.Ref), y=max_pots_2) + geom_boxplot(outliers=F) + facet_wrap(~Analysis.Area)


#cpue and vessel count
ggplot(D7) + aes(x=factor(vessel_count_mgmt_u), y=log(CPUE_nom)) + geom_boxplot()
ggplot(D7) + aes(x=factor(vessel_count_mgmt_u), y=log(CPUE_nom)) + geom_boxplot(outliers=F)
ggplot(D7) + aes(x=factor(vessel_count_mgmt_u), y=log(CPUE_nom)) + geom_boxplot(outliers=F) + facet_wrap(~Analysis.Area)

#cpue and district


#cpue and jdate



#correlation plot
library(corrplot)
D7_interest_cor <- D7 %>% select(CPUE_nom, jdate, vessel_count_mgmt_u)
cor_prep <- cor(D7_interest_cor)
corrplot(cor_prep)
?pairs


#check model residuals, see if we need ar1 correlation in the model
```


Drop the outlier, perhaps
```{r}
D7 <- D7 %>% filter(CPUE_nom < 100) #gonna assume that is an outlier
```


Remove ADFG vessel from consideration (add this to shrimp prep function later)
```{r}
D7 <- D7 %>% filter(ADFG.Number!= 99999)
unique(D7$ADFG.Number)
```



Model selection 5/15/24
```{r}

#ok can breifly test ranefs
M_ranef <-  lmer(log(CPUE_nom+0.001) ~ Season.Ref + vessel_count_aa + Season.Ref:Analysis.Area + Analysis.Area + (1|ADFG.Number), data=D7, REML=T)


M_noranef <-  lm(log(CPUE_nom+0.001) ~ Season.Ref +vessel_count_aa + ADFG.Number + Season.Ref:Analysis.Area + Analysis.Area, data=D7)
AIC(M_ranef, M_noranef) #no ranefs

summary(M_ranef)



#global with ranef and interactions

M2 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + vessel_count_aa + ADFG.Number +Analysis.Area + Season.Ref:Analysis.Area, data=D7)

M3 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + ADFG.Number + Analysis.Area + Season.Ref:Analysis.Area, data=D7)

M4 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + ADFG.Number +Analysis.Area, data=D7)

M5 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + Analysis.Area + Season.Ref:Analysis.Area, data=D7)

M_null <- lm(log(CPUE_nom+0.001) ~ Season.Ref+ Analysis.Area, data=D7)

AIC(M2, M3, M4, M5, M_null) #M3 wins
#says that vessel count does not matter...


summary(M3)

```





Another imputation method: LOCF- last obervation carried forwards - might work well for cpue
```{r}
  ############################################
  #I asked the internet for some help and:
  # Step 1: Calculate the mean logCPUE for each season
season_means <- D7_sub3 %>%
  group_by(Season.Ref, Analysis.Area) %>%
  summarise(mean_logCPUE = mean(logCPUE, na.rm = TRUE)) %>%
  arrange(Season.Ref, Analysis.Area)
  
  # Step 2: Join the season means back to the original dat
  D7_filled_2 <- D7_sub3 %>%
  left_join(season_means)
  
  # Step 3: Fill the NA values with the mean from the previous season
D7_filled_2 <- D7_filled_2 %>%
  arrange(Season.Ref) %>%
  group_by(Analysis.Area) %>%
  mutate(logCPUE = if_else(is.na(logCPUE),
                           dplyr::lag(mean_logCPUE),
                           logCPUE)) %>%
  ungroup()

# Drop the mean_logCPUE column as it's no longer needed
D7_filled_2 <- D7_filled_2 %>%
  select(-mean_logCPUE)

ggplot(D7_filled_2) + aes(x=Season.Ref, y=logCPUE) + geom_point() + facet_wrap(~Analysis.Area)

  #ok, that looks good,
  
  
```



5/28/24: weights on areas (the size of the analysis areas when modeling!! see if you can remove the bias of additional data)
Model selection after imputation
```{r}
#D7_filled_2 is my dataframe
#import areas of .... stat areas
## from an R standpoint, it will be messy and full of NA's
areas_raw <- read.csv("Data/Southeast_Shrimp_StatArea_SqMiles.csv")
names(areas_raw)
areas_D7 <- areas_raw %>% #from the messy dataset
  select(DISTRICT_CODE, STAT_AREA_NAME, STAT_AREA, Area_SqMiles) %>% #choose the columns I care about
  filter(DISTRICT_CODE == 107) #%>% #I only care about District 7 right now
  #group_by(S)

View(areas_D7)

U_ern_sound <- areas_D7 %>% filter(STAT_AREA == 10720) %>%
  summarise(area=sum(Area_SqMiles))
L_ern_sound <- areas_D7 %>% filter(STAT_AREA == 10710) %>%
  summarise(area=sum(Area_SqMiles))
Zim_st <- areas_D7 %>% filter(STAT_AREA == 10730|STAT_AREA == 10735) %>%
  summarise(area=sum(Area_SqMiles))
Brad_Can <- areas_D7 %>% filter(STAT_AREA == 10740|STAT_AREA == 10745) %>%
  summarise(area=sum(Area_SqMiles))

df_temp <- data.frame(Analysis.Area= c("Upper Ernest Sound", "Lower Ernest Sound", "Bradfield Canal", "Zimovia Strait"),
                      area_sqmi = c(U_ern_sound$area, L_ern_sound$area, Zim_st$area, Brad_Can$area))

#ugh that was tedious
##at least I have my areas now

D7_filled_3 <- left_join(D7_filled_2, df_temp) #joined by analysis areas, now I have areas that I can use to weigh


#now model selection, using areas as the weights. MAybe I do that step when calculating... mgmt unit cpue, and I should run the model without weights for pred cpue
##should I weigh time, somehow...?

#anyway, generate a model
M3_2 <- lm(logCPUE~ Season.Ref + ADFG.Number + Analysis.Area + Season.Ref:Analysis.Area, data=D7_filled_3, weights=area_sqmi) #shold I weigh the diff years at all??
summary(M3_2)
M_alt1 <- lm(logCPUE~ Season.Ref + ADFG.Number + Analysis.Area, data=D7_filled_3, weights=area_sqmi)
M_alt2 <- lm(logCPUE~ Season.Ref + Analysis.Area + Season.Ref:Analysis.Area, data=D7_filled_3, weights=area_sqmi) 

AIC(M3_2, M_alt1, M_alt2)

#ok but what about time? Should I add time in there, weigh a row of data less if there are many observations for that year and analysis area?
##weights could be: area_sq* (1/total#) of obs per that Analysis.Area and Season.Ref combo. Does that make sense???

```




Predict 5/15/24
```{r}
table(D7$ADFG.Number)


std_dat<- expand.grid(Season.Ref = unique(D7_filled_3$Season.Ref), #do I need to extrapolate for areas/years taht do not exist??
                              Analysis.Area = unique(D7_filled_3$Analysis.Area), #does not like east behm canal
                              #Hooks = round(mean(cpue_dat$Hooks),0), 
                              #Line_no = 2, #mean(cpue_dat$Line_no) unique(cpue_dat$Line_no)
                              ADFG.Number = factor(52131) #52131 appears 399 times.
                      )

pred_logcpue <- predict(M3_2, std_dat, type = "response", se = TRUE) 

#cool, I have a predicted log cpue

```


Standardized (predicted)  CPUE
(from internet: Note I predict the log version as exp(predict(m) + sigma^2/2) because $E exp(X) = exp(mu + sigma^2)$ where $X \sim N(0, sigma^2)$. (This changes little here though.))
See: https://stats.stackexchange.com/questions/115571/back-transforming-regression-results-when-modeling-logy
```{r}
?predict.merMod
pred<- predict(m_glob6_IR, type = "response", se = TRUE) #newdata needs something?? right?

#newdata
newdata <- expand.grid(Season.Ref = unique(D7$Season.Ref),
                               vessel_count_aa = round(mean(D7$vessel_count_aa),0),   #  unique(corr_spot_limited$jdate)... might need to expand and average this over all dates too
                               ADFG.Number = unique(D7$ADFG.Number), #table(corr_spot$ADFG.Number) #52131 #56332 #41228
                              Analysis.Area = unique(D7$Analysis.Area)
                               
 )

pred1 <- predict(m_glob6_IR, newdata, type = "response", se = TRUE)

pred_df <- data.frame(newdata, pred1$fit, pred1$se.fit)

#is this misleading? I have all vessels fising in all locations for this pred.
unique(D7 %>% filter(Analysis.Area=="Bradfield Canal") %>% select(ADFG.Number)) #30
unique(D7 %>% filter(Analysis.Area=="Lower Ernest Sound") %>% select(ADFG.Number)) #39
unique(D7 %>% filter(Analysis.Area=="Upper Ernest Sound") %>% select(ADFG.Number)) #51
unique(D7 %>% filter(Analysis.Area=="Zimovia Strait") %>% select(ADFG.Number)) #30
unique(D7$ADFG.Number) #88
###so, this might not be the best predict. Maybe I should leave the vessel part out, or choose a vessel in the middle?
###also, filter out unknown vessel at some point please

#what is, like, a very average fishing vessel, that is in all locations?
#or I could use jsut the fishing vessels that overlap? WHAT DID PHIL AND TYLER DO?!?!?!?!
###oh. see Spot_shrimp_ernest_clean_4.R code for an option.starts at 894. I weigh the ADFG.Number based on how many timest it appears in the dataset. How does that work for multiple areas tho??


#newdata
newdata2 <- expand.grid(Season.Ref = unique(D7$Season.Ref),
                               vessel_count_aa = round(mean(D7$vessel_count_aa),0),   #  unique(corr_spot_limited$jdate)... might need to expand and average this over all dates too
                              # ADFG.Number = unique(D7$ADFG.Number), #table(corr_spot$ADFG.Number) #52131 #56332 #41228
                              Analysis.Area = unique(D7$Analysis.Area)
                               
 )

pred2 <- predict(m_glob6_IR, newdata2, type = "response", se = TRUE) #ok so adfg numver should be something


#also, remember that exp() will give you the mean, so use the adjustment to get the median plz!!!!
```




Analysis and model testing: individual analysis areas
```{r}
#Bradfield canal

#Lower ernest sound

#upper ernest sound

#Zimovia strait


```

Comparison graphs
```{r}

```

Results graphs
```{r}

```

Aside:
IS ADFG vessel accounted for?
-in email max says:

This is where shrimp survey data is located.
https://www.adfg.alaska.gov/analytics/saw.dll?Answers&path=%2Fshared%2FCommercial%20Fisheries%2FRegion%20I%2FInvertebrates%2FSurvey%2FShrimp%2FSpecimens%20with%20Pot%20Data

Survey catches are also reported in fish ticket data, recorded as Fishery: Test Fish, and Permit Holder: ADFG Sitka. These survey catches should be removed from calculations in CPUE and total reportable harvest.- I DO NOT SEE THESE REPORTED THO


_______

Take away from KAtie meeting:
focus on shrimp ticket for now.
Ask Max at what level does he make decisions. When he closes an area, what is that based off of?

