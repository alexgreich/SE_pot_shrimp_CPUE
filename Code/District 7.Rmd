---
title: "District 7"
author: "Alex Reich"
date: "2024-04-24"
output: html_document
---
Aside:
Argh, I had to upgrade to R 4.4.0 to get lme4 (the matrix package, specifically) to work. But now the r markdown settings are different and I can't default to the home directory being the project. So I will set that here:
```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/agreich/OneDrive - State of Alaska/Desktop/Shrimp/Shrimp tickets (pot shrimp)/SE_pot_shrimp_CPUE')
```



Anyway, let's get started. This document will look at management unit District 7 for pot (spot) shrimp standardization. It will load in the (pre-wrangled) data, wrangle further for district 1, and then some EDA (focusing on variables of interest and correlation). The analysis will be by district with an HGAM (hierarchical GAM) with the variables of interest as fixed effects and Analysis Area as a random effect. I will do model selection on a few versions of this HGAM. I will then make graphs of the standardized CPUE (by district and) and whatever else Max wants. THEN, I will run individual models by analysis areas, ideally with an automated function, and compare the graphs of those automated functions to the cpue of the ranef model.This will help me decide if models should be by MANAGEMENT UNIT (larger) or by ANALYSIS AREA (smaller). In hindsight, I should start with District 7, since I have explored the Upper Ernest sound data extensively.Ok, now the RMD is named accordingly 


Load libraries
```{r}
library(tidyverse)
library(ggplot2)
library(cowplot)
library(viridis)
library(RColorBrewer)
library(mgcv)
```


Load the data and select for D7
```{r}
wrangled_shrimp <- read.csv("Data/wrangled shrimp focus years.csv")
D7 <- wrangled_shrimp %>% filter(Management_unit == "District 7") %>%
  mutate(Analysis.Area = factor(Analysis.Area),
         ADFG.Number = factor(ADFG.Number),
         Season.Ref = factor(Season.Ref)
         )#make things factors for the gams

unique(D7$Analysis.Area)

```


District 7 contains analysis areas:
  Bradfield Canal
  Lower Ernest Sound
  Upper Ernest Sound
  Zimovia Strait
  
  
  
## Exploratory Data Analysis for D7
Variables of interest:
Random effect: Analysis.Area
Fixed effects: jdate (smoothed and... cyclical?), vessel # (the count of vessels), ADFG # (individual vessel (fixed or random effect??)), Season aka year, 
--If I do not include jdate, simplifies things?
Response: nominal cpue

Q: should I do something besides a log-transformation?
THIN OUT
```{r}

#cpue dist
ggplot(D7) + aes(x=CPUE_nom) + geom_density()
ggplot(D7) + aes(x=log(CPUE_nom+0.001)) + geom_density()
qqnorm(log(D7$CPUE_nom)) #that kind of looks bad. Maybe somehting other than log-trans?
qqnorm(D7$CPUE_nom)

ggplot(D7) + aes(x=CPUE_nom) + geom_density()+facet_wrap(~Analysis.Area)
ggplot(D7) + aes(x=log(CPUE_nom+0.001)) + geom_density()+facet_wrap(~Analysis.Area)

#cpue by year
ggplot(D7) + aes(x=factor(Season.Ref), y=CPUE_nom) + geom_point() #ther is a big outlier
ggplot(D7) + aes(x=factor(Season.Ref), y=CPUE_nom) + geom_boxplot(outliers = F)

ggplot(D7) + aes(x=factor(Season.Ref), y=CPUE_nom) + geom_boxplot(outliers = F) + facet_wrap(~Analysis.Area) #cpue varies... cyclically with time. Is season a fixed effect? I want to estimate it, so probs not. its def not linear.

#catch by year
ggplot(D7) + aes(x=factor(Season.Ref), y=total_weight) + geom_boxplot() #this is total weight per fish ticket.
ggplot(D7) + aes(x=factor(Season.Ref), y=max_pots_2) + geom_boxplot()
ggplot(D7) + aes(x=factor(Season.Ref), y=max_pots_2) + geom_boxplot(outliers=F)

ggplot(D7) + aes(x=factor(Season.Ref), y=total_weight) + geom_boxplot() + facet_wrap(~Analysis.Area) #max pots per fish ticket
ggplot(D7) + aes(x=factor(Season.Ref), y=max_pots_2) + geom_boxplot(outliers=F) + facet_wrap(~Analysis.Area)


#cpue and vessel count
ggplot(D7) + aes(x=factor(vessel_count_mgmt_u), y=log(CPUE_nom)) + geom_boxplot()
ggplot(D7) + aes(x=factor(vessel_count_mgmt_u), y=log(CPUE_nom)) + geom_boxplot(outliers=F)
ggplot(D7) + aes(x=factor(vessel_count_mgmt_u), y=log(CPUE_nom)) + geom_boxplot(outliers=F) + facet_wrap(~Analysis.Area)

#cpue and district


#cpue and jdate



#correlation plot
library(corrplot)
D7_interest_cor <- D7 %>% select(CPUE_nom, jdate, vessel_count_mgmt_u)
cor_prep <- cor(D7_interest_cor)
corrplot(cor_prep)
?pairs


#check model residuals, see if we need ar1 correlation in the model
```


Drop the outlier, perhaps
```{r}
D7 <- D7 %>% filter(CPUE_nom < 100) #gonna assume that is an outlier
```


Remove ADFG vessel from consideration (add this to shrimp prep function later)
```{r}
D7 <- D7 %>% filter(ADFG.Number!= 99999)
unique(D7$ADFG.Number)
```



Model selection 5/15/24
```{r}

#ok can breifly test ranefs
M_ranef <-  lmer(log(CPUE_nom+0.001) ~ Season.Ref + vessel_count_aa + Season.Ref:Analysis.Area + Analysis.Area + (1|ADFG.Number), data=D7, REML=T)


M_noranef <-  lm(log(CPUE_nom+0.001) ~ Season.Ref +vessel_count_aa + ADFG.Number + Season.Ref:Analysis.Area + Analysis.Area, data=D7)
AIC(M_ranef, M_noranef) #no ranefs

summary(M_ranef)



#global with ranef and interactions

M2 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + vessel_count_aa + ADFG.Number +Analysis.Area + Season.Ref:Analysis.Area, data=D7)

M3 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + ADFG.Number + Analysis.Area + Season.Ref:Analysis.Area, data=D7)

M4 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + ADFG.Number +Analysis.Area, data=D7)

M5 <- lm(log(CPUE_nom+0.001) ~ Season.Ref + Analysis.Area + Season.Ref:Analysis.Area, data=D7)

M_null <- lm(log(CPUE_nom+0.001) ~ Season.Ref+ Analysis.Area, data=D7)

AIC(M2, M3, M4, M5, M_null) #M3 wins
#says that vessel count does not matter...


summary(M3)

```





Another imputation method: LOCF- last obervation carried forwards - might work well for cpue
```{r}
D7$logCPUE <- log(D7$CPUE_nom) #I do not need the +0.001 becayse everything is positive
#lets try imputing data for area:year wihtout imputing for every single damn boat
temp2 <- expand.grid(Season.Ref = unique(D7$Season.Ref),
                     Analysis.Area = unique(D7$Analysis.Area)
)
D7_sub <- D7 %>% select(Season.Ref, Analysis.Area, ADFG.Number, logCPUE)
D7_sub2 <- full_join(D7_sub, temp2)
#make missing values for ADFG.Number the mode
which.max(table(D7_sub2$ADFG.Number)) #52131

mode_boat <- which.max(table(D7$ADFG.Number))

mode_val <- names(mode_boat)
  


D7_sub3 <- D7_sub2 %>%
  #mutate(ADFG.Number = ifelse(is.na(ADFG.Number), mode_val, ADFG.Number)) #replacing NA with the mode fishing vessel
  mutate(ADFG.Number = replace_na(ADFG.Number, mode_val))
############################################
  #I asked the internet for some help and:
  # Step 1: Calculate the mean logCPUE for each season
season_means <- D7_sub3 %>%
  group_by(Season.Ref, Analysis.Area) %>%
  summarise(mean_logCPUE = mean(logCPUE, na.rm = TRUE)) %>%
  arrange(Season.Ref, Analysis.Area)
  
  # Step 2: Join the season means back to the original dat
  D7_filled_2 <- D7_sub3 %>%
  left_join(season_means)
  
  # Step 3: Fill the NA values with the mean from the previous season
D7_filled_2 <- D7_filled_2 %>%
  arrange(Season.Ref) %>%
  group_by(Analysis.Area) %>%
  mutate(logCPUE = if_else(is.na(logCPUE),
                           dplyr::lag(mean_logCPUE),
                           logCPUE)) %>%
  ungroup()

# Drop the mean_logCPUE column as it's no longer needed
D7_filled_2 <- D7_filled_2 %>%
  select(-mean_logCPUE)

ggplot(D7_filled_2) + aes(x=Season.Ref, y=logCPUE) + geom_point() + facet_wrap(~Analysis.Area)

  #ok, that looks good,
  
  
```



5/28/24: weights on areas (the size of the analysis areas when modeling!! see if you can remove the bias of additional data)
Model selection after imputation
```{r}
#D7_filled_2 is my dataframe
#import areas of .... stat areas
## from an R standpoint, it will be messy and full of NA's
areas_raw <- read.csv("Data/Southeast_Shrimp_StatArea_SqMiles.csv")
names(areas_raw)
areas_D7 <- areas_raw %>% #from the messy dataset
  select(DISTRICT_CODE, STAT_AREA_NAME, STAT_AREA, Area_SqMiles) %>% #choose the columns I care about
  filter(DISTRICT_CODE == 107) #%>% #I only care about District 7 right now
  #group_by(S)

View(areas_D7)

U_ern_sound <- areas_D7 %>% filter(STAT_AREA == 10720) %>%
  summarise(area=sum(Area_SqMiles))
L_ern_sound <- areas_D7 %>% filter(STAT_AREA == 10710) %>%
  summarise(area=sum(Area_SqMiles))
Zim_st <- areas_D7 %>% filter(STAT_AREA == 10730|STAT_AREA == 10735) %>%
  summarise(area=sum(Area_SqMiles))
Brad_Can <- areas_D7 %>% filter(STAT_AREA == 10740|STAT_AREA == 10745) %>%
  summarise(area=sum(Area_SqMiles))

df_temp <- data.frame(Analysis.Area= c("Upper Ernest Sound", "Lower Ernest Sound", "Bradfield Canal", "Zimovia Strait"),
                      area_sqmi = c(U_ern_sound$area, L_ern_sound$area, Zim_st$area, Brad_Can$area))

#ugh that was tedious
##at least I have my areas now

D7_filled_3 <- left_join(D7_filled_2, df_temp) #joined by analysis areas, now I have areas that I can use to weigh


#now model selection, using areas as the weights. MAybe I do that step when calculating... mgmt unit cpue, and I should run the model without weights for pred cpue
##should I weigh time, somehow...?

#anyway, generate a model
M3_2 <- lm(logCPUE~ Season.Ref + ADFG.Number + Analysis.Area + Season.Ref:Analysis.Area, data=D7_filled_3, weights=area_sqmi) #shold I weigh the diff years at all??
summary(M3_2)
M_alt1 <- lm(logCPUE~ Season.Ref + ADFG.Number + Analysis.Area, data=D7_filled_3, weights=area_sqmi)
M_alt2 <- lm(logCPUE~ Season.Ref + Analysis.Area + Season.Ref:Analysis.Area, data=D7_filled_3, weights=area_sqmi) 

AIC(M3_2, M_alt1, M_alt2)

#ok but what about time? Should I add time in there, weigh a row of data less if there are many observations for that year and analysis area?
##weights could be: area_sq* (1/total#) of obs per that Analysis.Area and Season.Ref combo. Does that make sense???

```




Predict 5/30/24
- shoudld I add weights to the prediction function? Try it and see if there is a change.
```{r}
table(D7$ADFG.Number)


std_dat<- expand.grid(Season.Ref = unique(D7_filled_3$Season.Ref),
                              Analysis.Area = unique(D7_filled_3$Analysis.Area), 
                              ADFG.Number = factor(52131) 
                      )

pred_logcpue <- predict(M3_2, std_dat, type = "response", se = TRUE) #analysis areas are weighed
#should I add weights to predict?? for area?

#cool, I have a predicted log cpue

#get the unlogged cpue and put back in DF
ln_mu = pred_logcpue$fit
ln_sigma = pred_logcpue$se.fit


#std_dat %>% 
 #  mutate(
  #        log_cpue_pred=fit,
   #       bt_cpue = exp(fit), 
    #      bt_upper = exp(upper),
     #     bt_lower = exp(lower),
      #    bt_se = exp(se)
         # bt_cv = bt_se/bt_cpue
  # ) -> pred_dat

std_dat %>% 
   mutate(
          ln_mu=ln_mu,
          mu = exp(ln_mu + (ln_sigma^2)/2),
          sigma = sqrt((exp(ln_sigma^2) - 1) * exp(2*ln_mu + ln_sigma^2)), #make sure is right
          upper= mu + 2*sigma,
          lower = mu - 2*sigma
   ) -> pred_dat_biascorrected



##### bias corrected
```

From tyler code:
# f_bias_correct () ----
# bias correction from log space
# args: ln_mu = mean of log(x)
#       ln_sigma = standard error of log(x)
f_bias_correct <- function(ln_mu, ln_sigma) {
  
  mu = exp(ln_mu + (ln_sigma^2)/2)
  sigma = sqrt((exp(ln_sigma^2) - 1) * exp(2*ln_mu + ln_sigma^2))
  
  return(c(mu = mu, sigma = sigma))
}


Standardized (predicted)  CPUE
(from internet: Note I predict the log version as exp(predict(m) + sigma^2/2) because $E exp(X) = exp(mu + sigma^2)$ where $X \sim N(0, sigma^2)$. (This changes little here though.))
See: https://stats.stackexchange.com/questions/115571/back-transforming-regression-results-when-modeling-logy





Analysis and model testing: individual analysis areas
```{r}
#Bradfield canal

#Lower ernest sound

#upper ernest sound

#Zimovia strait


```

Comparison graphs
```{r}
ggplot(pred_dat_biascorrected) + aes(x=Season.Ref, y=mu) +geom_errorbar(aes(ymin=lower, ymax=upper))+ geom_point() + geom_line(aes(group=Analysis.Area)) + facet_wrap(~Analysis.Area)
```

Results graphs
```{r}

```

Aside:
IS ADFG vessel accounted for?
-in email max says:

This is where shrimp survey data is located.
https://www.adfg.alaska.gov/analytics/saw.dll?Answers&path=%2Fshared%2FCommercial%20Fisheries%2FRegion%20I%2FInvertebrates%2FSurvey%2FShrimp%2FSpecimens%20with%20Pot%20Data

Survey catches are also reported in fish ticket data, recorded as Fishery: Test Fish, and Permit Holder: ADFG Sitka. These survey catches should be removed from calculations in CPUE and total reportable harvest.- I DO NOT SEE THESE REPORTED THO


_______

Take away from KAtie meeting:
focus on shrimp ticket for now.
Ask Max at what level does he make decisions. When he closes an area, what is that based off of?

